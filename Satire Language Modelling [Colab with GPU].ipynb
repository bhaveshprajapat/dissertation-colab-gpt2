{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Satire Language Modelling [Colab GPU]",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaveshprajapat/dissertation-colab-gpt2/blob/main/Satire%20Language%20Modelling%20%5BColab%20with%20GPU%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "#Language Modelling in Google Colab (Pro) with *GPUs*\n",
        "Bhavesh Prajapat,\n",
        "adapted from [Train a GPT-2 Text-Generating Model w/ GPU](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "This Colab notebook shows the process taken for generating fine-tuned models using the satire dataset produced as part of my dissertation project '‘Investigating\n",
        "Language Modelling Suitability for Originating Satire'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVn82yVFpcjo"
      },
      "source": [
        "## Fine-tuning stage\n",
        "Sets up the Colab VM, downloads a GPT-2 model, and readies the dataset to fine-tune with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xo9wRNVokld"
      },
      "source": [
        "### Setup\n",
        "Sets TF version, and displays GPU info.\n",
        "\n",
        "The size of the GPU Memory is important for determining which models you can run. Use Colab Pro for reliable access to higher-memory GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TOba33L4qf",
        "outputId": "2852f1dc-e815-4bdb-c712-0eaeaec2c0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Sun May 10 16:10:22 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e_cwK2BIuyi"
      },
      "source": [
        "Install  PIP requirements and mount  Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHDG_EM2IqZ5",
        "outputId": "0d861b5e-a104-4b70-ccd8-b218154a39d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS-ke_fxjpcH",
        "outputId": "c5b27103-93f1-4629-9eaf-395de098a411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Skip if Google Drive is already mounted\n",
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gx_1xV-Irk8"
      },
      "source": [
        "### Runtime Constants\n",
        "model_size is one of:\n",
        "*   `124M` (S, 1.5GB)\n",
        "*   `355M` (M)\n",
        "*   `774M` (L)\n",
        "*   `1558M` (XL, 6.5GB)\n",
        "\n",
        "L and XL are unlikely to work well with lower-memory GPUs\n",
        "\n",
        "Optionally, copy an existing checkpoint from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTu-FxyhI8S2"
      },
      "source": [
        "model_size = \"124M\" #@param [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
        "sess_run_name = \"GPT2S-A-SATIRE-500\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARa5W6vkPukU"
      },
      "source": [
        "# Comment out if unneeded/existing checkpoint doesn't exist\n",
        "gpt2.copy_checkpoint_from_gdrive(\"GPT2L-A-SATIRE-500\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ9wIRdfJGba"
      },
      "source": [
        "### Load Dataset\n",
        "The satire training dataset `DatasetA.zip` must be loaded in to the Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssHOZRDLy5W",
        "outputId": "24d92525-6765-4c85-bfd9-3652adc4d5fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Comment and uncomment as necessary\n",
        "!rm -rf DatasetA\n",
        "!cp \"/content/drive/My Drive/DatasetA.zip\" .\n",
        "!unzip -q DatasetA.zip -d DatasetA\n",
        "# Set the dataset folder name\n",
        "folder_name = \"/content/DatasetA\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/DatasetA.zip': No such file or directory\n",
            "unzip:  cannot find or open DatasetA.zip, DatasetA.zip.zip or DatasetA.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX-WdkqRJbf-"
      },
      "source": [
        "### Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/[sess_run_name]` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLHOV-thJfQS",
        "cellView": "both"
      },
      "source": [
        "#@title Fine-tuning parameters\n",
        "\n",
        "#@markdown Not all of the parameters listed above are included in the form.\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_size)\n",
        "\n",
        "import tensorflow as tf # Import tf library directly\n",
        "tf.reset_default_graph() # Allows this cell to be re-run without VM restart\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "# gpt2.load_gpt2(sess, run_name=sess_run_name) # Load a backed-up checkpoint from Google Drive\n",
        "dataset=folder_name\n",
        "#@markdown Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "restore_from = \"fresh\" #@param [\"fresh\", \"latest\"] {allow-input: true}\n",
        "\n",
        "#@markdown Number of fine-tuning steps to take\n",
        "steps =  500#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Number of steps to print training progress.\n",
        "print_every=10#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Number of steps to print example output\n",
        "sample_every=500#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Number of fine-tuning steps to take\n",
        "save_every=1000#@param {type:\"integer\"}\n",
        "gpt2.finetune(sess,\n",
        "              dataset=folder_name,\n",
        "              steps=steps,\n",
        "              restore_from=restore_from,\n",
        "              run_name=sess_run_name,\n",
        "              print_every=print_every,\n",
        "              model_name=model_size,\n",
        "              sample_every=sample_every,\n",
        "              save_every=save_every\n",
        "              )\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name=sess_run_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWjw2EutJlUS"
      },
      "source": [
        "---\n",
        "## Text-generation Stage\n",
        "\n",
        "This immediately follows on from the previous Fine-tuning stage. However, `sess_run_name` can be the name of any model which has been backed up to Google Drive, not necessarily a model that has just been constructed in the above steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XzyIrm2x3QN"
      },
      "source": [
        "### Setup, and set text-generation parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9PtarQtJpu1",
        "cellView": "both"
      },
      "source": [
        "# Copy a checkpoint to the Colab Runtime\n",
        "!cp -r \"drive/My Drive/Saved Colab Checkpoints/sess_run_name\" checkpoint/sess_run_name\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name=sess_run_name)\n",
        "\n",
        "#@markdown Length of samples in characters\n",
        "text_length =  300#@param {type:\"number\"}\n",
        "#@markdown The higher the temperature, the 'crazier' the text.\n",
        "gen_temp = 0.7 #@param {type:\"slider\", min:0.7, max:1.0, step:0.01}\n",
        "#@markdown Makes text sample generations conditional on a set prefix.\n",
        "text_prefix = \"Insert text here\" #@param {type:\"string\"}\n",
        "#@markdown Number of samples to generate \n",
        "nsamp =  5#@param {type:\"number\"}\n",
        "#@markdown Batch size (samples to generate in parallel, can increase speed of generation)\n",
        "batch_s =  5#@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egFTmfhjx9xT"
      },
      "source": [
        "### Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLl3UUrAxtL0"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              run_name=sess_run_name,\n",
        "              length=text_length,  \n",
        "              temperature=gen_temp, \n",
        "              prefix=text_prefix,\n",
        "              nsamples=nsamp,\n",
        "              batch_size=batch_s \n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST_BHP3BJs8H"
      },
      "source": [
        "### Text generation in bulk\n",
        "\n",
        "You can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAHYwpJcJtUH"
      },
      "source": [
        "#@title Text-generation parameters\n",
        "\n",
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "#@markdown Length of samples in characters\n",
        "text_length =  300#@param {type:\"number\"}\n",
        "#@markdown The higher the temperature, the 'crazier' the text.\n",
        "gen_temp = 0.7 #@param {type:\"slider\", min:0.7, max:1.0, step:0.01}\n",
        "#@markdown Number of samples to generate \n",
        "nsamp =  5#@param {type:\"number\"}\n",
        "#@markdown Batch size (samples to generate in parallel, can increase speed of generation)\n",
        "batch_s =  5#@param {type:\"number\"}\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=text_length,\n",
        "                      temperature=gen_temp,\n",
        "                      nsamples=nsamp,\n",
        "                      batch_size=batch_s\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFz_DkfpJvpa"
      },
      "source": [
        "# Youmay have to run this cell twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}