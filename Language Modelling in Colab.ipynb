{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dissertation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaveshprajapat/dissertation-colab-gpt2/blob/main/Language%20Modelling%20in%20Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "#Language Modelling in Colab Pro\n",
        "Based off of code from Max Woolf (https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TOba33L4qf",
        "outputId": "2852f1dc-e815-4bdb-c712-0eaeaec2c0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Sun May 10 16:10:22 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e_cwK2BIuyi"
      },
      "source": [
        "# Setup\n",
        "Install the PIP requirements and mount the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHDG_EM2IqZ5",
        "outputId": "0d861b5e-a104-4b70-ccd8-b218154a39d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS-ke_fxjpcH",
        "outputId": "c5b27103-93f1-4629-9eaf-395de098a411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gx_1xV-Irk8"
      },
      "source": [
        "# Runtime Constants\n",
        "model_size is one of:\n",
        "*   124M (S)\n",
        "*   355M (M)\n",
        "*   774M (L)\n",
        "*   355M (XL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTu-FxyhI8S2"
      },
      "source": [
        "model_size = \"124M\"\n",
        "sess_run_name = \"GPT2S-A-SATIRE-500\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARa5W6vkPukU"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(\"GPT2L-A-SATIRE-500\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ9wIRdfJGba"
      },
      "source": [
        "# Download Satire Datasets\n",
        "Downloads the ready made satire dataset from the *GitLab*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssHOZRDLy5W"
      },
      "source": [
        "# Comment and uncomment as necessary\n",
        "!rm -rf DatasetA\n",
        "!cp \"/content/drive/My Drive/DatasetA.zip\" .\n",
        "!unzip -q DatasetA.zip -d DatasetA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBESltEK9wXO"
      },
      "source": [
        "#!tar -xf /content/satire-datasets/satire-train.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09tt3PgiOAYq"
      },
      "source": [
        "# Set the dataset file name\n",
        "folder_name = \"/content/DatasetA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX-WdkqRJbf-"
      },
      "source": [
        "# Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLHOV-thJfQS",
        "outputId": "f6dbcf93-1470-4d5f-d919-4cf4d5eeb2e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=model_size)\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "#gpt2.load_gpt2(sess, run_name='GPT2L-A-SATIRE-500')\n",
        "gpt2.finetune(sess,\n",
        "              dataset=folder_name,\n",
        "              steps=500,\n",
        "              restore_from='fresh',\n",
        "              run_name=\"GPT2S-A-SATIRE-500\",\n",
        "              print_every=10,\n",
        "              model_name=\"124M\",\n",
        "              sample_every=500,\n",
        "              save_every=1000\n",
        "              )\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name=sess_run_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 297Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 93.4Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 848Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 234Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 292Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 144Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 156Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/26810 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26810/26810 [00:59<00:00, 448.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 10178271 tokens\n",
            "Training...\n",
            "[10 | 21.11] loss=3.09 avg=3.09\n",
            "[20 | 33.75] loss=3.20 avg=3.15\n",
            "[30 | 46.49] loss=3.34 avg=3.21\n",
            "[40 | 59.19] loss=3.16 avg=3.20\n",
            "[50 | 71.86] loss=3.21 avg=3.20\n",
            "[60 | 84.55] loss=3.13 avg=3.19\n",
            "[70 | 97.20] loss=3.28 avg=3.20\n",
            "[80 | 109.94] loss=3.12 avg=3.19\n",
            "[90 | 122.62] loss=3.25 avg=3.20\n",
            "[100 | 135.29] loss=3.24 avg=3.20\n",
            "[110 | 148.02] loss=3.10 avg=3.19\n",
            "[120 | 160.67] loss=2.89 avg=3.17\n",
            "[130 | 173.32] loss=3.31 avg=3.18\n",
            "[140 | 185.94] loss=3.21 avg=3.18\n",
            "[150 | 198.58] loss=3.12 avg=3.18\n",
            "[160 | 211.22] loss=3.26 avg=3.18\n",
            "[170 | 223.89] loss=3.38 avg=3.20\n",
            "[180 | 236.55] loss=3.20 avg=3.20\n",
            "[190 | 249.15] loss=3.01 avg=3.19\n",
            "[200 | 261.75] loss=3.13 avg=3.18\n",
            "[210 | 274.42] loss=3.36 avg=3.19\n",
            "[220 | 287.08] loss=3.34 avg=3.20\n",
            "[230 | 299.75] loss=3.55 avg=3.22\n",
            "[240 | 312.38] loss=3.23 avg=3.22\n",
            "[250 | 324.98] loss=3.04 avg=3.21\n",
            "[260 | 337.64] loss=3.08 avg=3.20\n",
            "[270 | 350.33] loss=3.16 avg=3.20\n",
            "[280 | 362.97] loss=3.09 avg=3.20\n",
            "[290 | 375.66] loss=2.91 avg=3.19\n",
            "[300 | 388.31] loss=3.00 avg=3.18\n",
            "[310 | 400.96] loss=3.06 avg=3.17\n",
            "[320 | 413.57] loss=3.13 avg=3.17\n",
            "[330 | 426.19] loss=3.21 avg=3.17\n",
            "[340 | 438.86] loss=3.08 avg=3.17\n",
            "[350 | 451.51] loss=3.34 avg=3.18\n",
            "[360 | 464.23] loss=3.21 avg=3.18\n",
            "[370 | 476.92] loss=3.24 avg=3.18\n",
            "[380 | 489.59] loss=3.02 avg=3.17\n",
            "[390 | 502.30] loss=3.09 avg=3.17\n",
            "[400 | 514.96] loss=3.01 avg=3.17\n",
            "[410 | 527.63] loss=3.18 avg=3.17\n",
            "[420 | 540.27] loss=3.08 avg=3.16\n",
            "[430 | 552.90] loss=3.38 avg=3.17\n",
            "[440 | 565.50] loss=3.18 avg=3.17\n",
            "[450 | 578.16] loss=3.31 avg=3.17\n",
            "[460 | 590.85] loss=3.22 avg=3.18\n",
            "[470 | 603.56] loss=3.01 avg=3.17\n",
            "[480 | 616.19] loss=3.07 avg=3.17\n",
            "[490 | 628.86] loss=3.10 avg=3.17\n",
            "[500 | 641.48] loss=2.80 avg=3.16\n",
            "Saving checkpoint/GPT2S-A-SATIRE-500/model-500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xSdQ1qOnErT",
        "outputId": "6ebfd271-1638-4739-f549-7cb79833c2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=\"GPT2L-A-SATIRE-5K\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0cfd7544514d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_checkpoint_to_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GPT2L-A-SATIRE-5K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mcopy_checkpoint_to_gdrive\u001b[0;34m(run_name, copy_folder)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# Reference: https://stackoverflow.com/a/17081026\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/My Drive/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, name, arcname, recursive, exclude, filter)\u001b[0m\n\u001b[1;32m   1957\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m                     self.add(os.path.join(name, f), os.path.join(arcname, f),\n\u001b[0;32m-> 1959\u001b[0;31m                             recursive, exclude, filter=filter)\n\u001b[0m\u001b[1;32m   1960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, name, arcname, recursive, exclude, filter)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mbltn_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36maddfile\u001b[0;34m(self, tarinfo, fileobj)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;31m# If there's data to follow, append it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m             \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1981\u001b[0m             \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9EJG2uhndL3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=\"GPT2L-A-SATIRE-3K\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tufe9mtOngNn"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=\"GPT2L-A-SATIRE-2K\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWjw2EutJlUS"
      },
      "source": [
        "# Generate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni4T7IT6JmV-"
      },
      "source": [
        "# Copy a checkpoint to the Colab Runtime\n",
        "#!cp -r \"drive/My Drive/Saved Colab Checkpoints/run-124M-COLAB\" checkpoint/run-124M-COLAB\n",
        "# import tensorflow as tf\n",
        "# tf.reset_default_graph()\n",
        "# sess = gpt2.start_tf_sess()\n",
        "# gpt2.load_gpt2(sess, run_name=sess_run_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9PtarQtJpu1"
      },
      "source": [
        "#@title Generation Parameters\n",
        "# text_length =  300#@param {type:\"number\"}\n",
        "# gen_temp = 0.7 #@param {type:\"slider\", min:0.7, max:1.0, step:0.01}\n",
        "# text_prefix = \"Gay Conversion Therapists Claim Most Patients Fully Straight By The Time They Commit Suicide\" #@param {type:\"string\"}\n",
        "# nsamp =  5#@param {type:\"number\"}\n",
        "# batch_s =  5#@param {type:\"number\"}\n",
        "# gpt2.generate(sess,\n",
        "#               run_name=sess_run_name,\n",
        "#               length=text_length,  \n",
        "#               temperature=gen_temp, \n",
        "#               prefix=text_prefix,\n",
        "#               nsamples=nsamp,\n",
        "#               batch_size=batch_s \n",
        "#               )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST_BHP3BJs8H"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAHYwpJcJtUH"
      },
      "source": [
        "#gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "#gpt2.generate_to_file(sess,\n",
        "#                      destination_path=gen_file,\n",
        "#                      length=500,\n",
        "#                      temperature=0.7,\n",
        "#                      nsamples=100,\n",
        "#                      batch_size=20\n",
        "#                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFz_DkfpJvpa"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "#files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}